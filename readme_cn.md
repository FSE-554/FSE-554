# Faithfulness of Reasoning Traces in LLM-based Code Vulnerability Detection

## ğŸ”° å®‰è£…

```bash
pip install -r requirements.txt
```

## âœ¨ æ•°æ®é›†å‡†å¤‡

æˆ‘ä»¬é€‰æ‹©äº†ä¹ä¸ªæ•°æ®é›†ï¼š [Devig](https://huggingface.co/datasets/DetectVul/devign)ã€ [BigVul](https://huggingface.co/datasets/bstee615/bigvul)ã€ [D2A](https://huggingface.co/datasets/claudios/D2A)ã€ [Draper](https://huggingface.co/datasets/claudios/Draper)ã€ [DiverseVulns](https://huggingface.co/datasets/NathanNeves/diversevulns)ã€ [REVEAL](https://huggingface.co/datasets/ivne20/reveal)ã€ [SVEN](https://huggingface.co/datasets/bstee615/sven)ã€ [VulDeePecker](https://huggingface.co/datasets/claudios/VulDeePecker)ã€ [PrimeVul](https://huggingface.co/datasets/colin/PrimeVul) ä»¥åŠåä¸ªæ¨¡å‹ï¼š[Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)ã€ [Phi-4](https://huggingface.co/microsoft/phi-4)ã€ [Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)ã€ [CodeLlama-Ins-7b](https://huggingface.co/AndreyRzhaksinskiy/CDS-CodeLlama-Ins-7b-E2E-20241022_baseline)ã€ [DeepSeek-R1-Distill-Llama-8B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)ã€ [DeepSeek-R1-Distill-Qwen-7B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)ã€ [DeepSeek-R1-Distill-Qwen-14B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)ã€ [NeuralExperiment-7b-MagicCoder-v5](https://huggingface.co/Kukedlc/NeuralExperiment-7b-MagicCoder-v5)ã€ [MPT-7B-StoryWriter](https://huggingface.co/mosaicml/mpt-7b-storywriter)ã€ [Qwen2.5-Coder-14B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct)ã€‚

é¦–å…ˆï¼Œæ‚¨éœ€è¦ä¸‹è½½æ‰€æœ‰æ•°æ®é›†å’Œæ‰€éœ€æ¨¡å‹ï¼Œé…ç½®å®ƒä»¬çš„è·¯å¾„ï¼Œå¹¶æå–æ ‡è®°ä¸ºä¸å®‰å…¨çš„ä»£ç æ®µã€‚æ•°æ®é›†ä¸­æ‰€æœ‰å¯¹è±¡çš„æ ¼å¼ä¸ºï¼š

```
```c{code}```
```

å…¶ä¸­ `code` æ˜¯æ•°æ®é›†ä¸­æ ‡è®°ä¸ºä¸å®‰å…¨çš„ä»£ç ã€‚

åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰æ–‡ä»¶è¿›è¡Œæ‹†åˆ†ï¼Œè¿™ç”±æ–‡ä»¶å¤¹ä¸­çš„ `divide.py` ä»£ç å¤„ç†ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

```
ç¤ºä¾‹ï¼š
åŸå§‹æ–‡ä»¶ï¼šcombined_full_patched_answer
æ‹†åˆ†æ–‡ä»¶ï¼šcombined_full_patched_answer_{i} // i çš„èŒƒå›´ä¸º 1 åˆ° 10
```

å¦‚æœæ‚¨ä¸éœ€è¦æ‹†åˆ†æ–‡ä»¶ï¼Œåˆ™éœ€è¦å°†åç»­ä»£ç ä¸­çš„è·¯å¾„ä¿®æ”¹ä¸ºæ•´ä¸ªæ–‡ä»¶çš„è·¯å¾„ã€‚

## ğŸ“ å®éªŒ

### RQ2: full-patch

é…ç½®æ•°æ®é›†å’Œæ‰€éœ€æ¨¡å‹åç§°ï¼Œå¹¶ä¿®æ”¹ Q1 æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰ä»£ç æ–‡ä»¶çš„è¾“å…¥å’Œè¾“å‡ºè·¯å¾„ã€‚

```
python run.py --combined_dataset "/path/to/data.json" --base_output_dir "/path/to/output"
python get_code.py --first_file "your_dataset_code.json" --results_root "your_out_put_dir" --combined_name "your_combined_name.json" --output_name "output.json"
python divide.py --root "your_base_dir" --input-name "your_data_all_answer.json" --prefix "your_data_all_answer_" --chunk-size 1000
python full_patched.py --api_key "your_api_key" --model_name "your_model" --base_url "https://your.api.url/" --input_file "your_input_file.json" --output_file "your_output_file.json"
python get_full_patched_data.py --base_dir "your_base_dir" --input_filename "your_input_filename.json" --output_filename "your_out_put_filename.json"
python run_full_patched.py --base_results_dir "/path/to/results" --input_basename "your_full_patched.json" --output_basename "your_output_basename.json"
python cul_full_patched.py --base_dir "your_base_directory_path" --models "model1" "model2" "model3"...
```

### RQ3: N-patch

é…ç½®æ•°æ®é›†å’Œæ‰€éœ€æ¨¡å‹åç§°ï¼Œå¹¶ä¿®æ”¹ RQ3 æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰ä»£ç æ–‡ä»¶çš„è¾“å…¥å’Œè¾“å‡ºè·¯å¾„ã€‚

```bash
1. python run.py --combined_dataset "/path/to/data.json" --base_output_dir "/path/to/output"
2. python get_code.py --first_file "your_dataset_code.json" --results_root "your_out_put_dir" --combined_name "your_combined_name.json" --output_name "output.json"
3. python divide.py --root "your_base_dir" --input-name "your_data_all_answer.json" --prefix "your_data_all_answer_" --chunk-size 1000
4. python run_N_patched.py --base_results_dir /path/to/results --input_basename your_full_patched_file_name.json --output_basename your_output_filename.json
5. python N_patched.py --base_url "your_base_url" --api_key "your_api_key" --model "your_model" --input_file "your_first_file_name.json" --output_file "your_output_filename.json"
6. python get_N_patched_data.py --base_dir "/path/to/base_dir" --input_filename "your_N_patched_file_name.json" --output_filename "your_output_file_name.json"
7. python del_N_patched.py --base_dir "/path/to/base_dir" --models model1 model2 --code_name "your_N_patched_code_{i}.json" --answer_name "your_N_patched_answer_{i}.json"
8. python your_script_name.py --base_dir "/path/to/base_dir" --models "model1" "model2"....
```

### RQ4: LoRA

ä½¿ç”¨ LoRA è°ƒæ•´æ¨¡å‹ã€‚æ„Ÿè°¢ [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„å·¥å…·æ¥å¾®è°ƒ LLMã€‚

æœ‰å…³è¯¦ç»†çš„ä½¿ç”¨è¯´æ˜ï¼Œè¯·å‚è€ƒ [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)ã€‚æˆ‘ä»¬å°†æä¾› `deepseek-qwen-7b` çš„è®­ç»ƒæ–‡ä»¶ç¤ºä¾‹ã€‚

é…ç½®æ‚¨çš„æ•°æ®é›†åŸºæœ¬ç›®å½•å¹¶æŒ‰é¡ºåºè¿è¡Œè„šæœ¬ï¼š

```bash
python data/scripts/prune_qwen_files.py --dir "/path/to/dataset_root" --apply
python data/scripts/organize_qwen_files.py --dir "/path/to/dataset_root" --apply
python data/scripts/filter_secure_in_origin_code.py --dir "/path/to/dataset_root/full_patched" --apply
python data/scripts/stat_qwen_pairs.py --base "/path/to/dataset_root"
python data/scripts/build_n_patched_faithfulness.py --dir "/path/to/dataset_root/N_patched" --out "/path/to/dataset_root/result/N_patched.json" --pretty
python data/scripts/build_full_patched_faithfulness.py --dir "/path/to/dataset_root/full_patched" --out "/path/to/dataset_root/result/full_patched.json" --pretty
python data/scripts/merge_dataset_to_base.py --base "/path/to/dataset_root/result" --out "/path/to/dataset_root/data_base.json" --pretty
```

åˆ‡æ¢åˆ°ç›¸åº”æ¨¡å‹çš„è„šæœ¬ç›®å½•å¹¶å¼€å§‹è®­ç»ƒï¼š

```bash
./run_train.fish
```

åœ¨ RQ4 ä¸­ä¿®æ”¹æ–‡ä»¶è·¯å¾„ä»¥å¯¹åº”äºåŸå§‹æ¨ç†ã€full-patchå’Œ N-patchã€‚

ç»ˆç«¯çª—å£ 1:

```bash
vllm serve \
    <MODEL_PATH> \
    --tensor-parallel-size <TP_SIZE> \
    --enable-lora \
    --lora-modules <LORA_NAME>=<LORA_CHECKPOINT_PATH> \
    --max-lora-rank <LORA_RANK> \
    --served-model-name <MODEL_NAME>
```

ç»ˆç«¯çª—å£ 2:

é‡å¤ RQ2 å’Œ RQ3 çš„è„šæœ¬é¡ºåºã€‚åœ¨ `run_lora.py` ä¸­ï¼Œè°ƒæ•´ç›¸åº”çš„æ¨¡å‹åç§°å’Œè·¯å¾„ã€‚å°† RQ2 å’Œ RQ3 ä¸­æ‰§è¡Œ `run.py` çš„å®ä¾‹æ›´æ”¹ä¸ºæ‰§è¡Œ `run_lora.py`ã€‚

```bash
python run_lora.py
```

